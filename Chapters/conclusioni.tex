\chapter{Conclusions}

In this work we explored the topic of how to perform a monitoring activity on self-driving cars, controlled by a Control System (Controller) and a System Supervisor (Safety-Monitor).

This study is meant to be a first step on the analysis of this kind of activity for such systems, which require particular care due to their complexity, and the critical issues of the environment in which it performs. Autonomous Vehicles themselves are not a new kind of systems, since they are commonly employed in military actions. However, the fact that self-driving cars work at close contact with people and that the urban environment requires extra care in handling the moltitude of events (hazardous or not) that may happen.
Moreover, we questioned if there are relationships between the effectiveness of the Safety-Monitor component and the Controller component, when the latter gets trained for long periods, using different strategies.

In the first sections, we reviewed the main concepts of these activities, such as

\begin{itemize}
	\item The definition of \textsl{dependability} and \textsl{safety}, which are central to Safety-Critical Systems, are seen in the light of self-driving cars
	\item The problems that prevent us to deploy self-driving cars in urban environments at this level of knowledge
	\item How a wrong/poor/optimistic \textsl{dependability assessment} of these systems may have tragic consequences when these systems are deployed
	\item The problems of studying the observed emergent behaviour resulting from the interaction of two main Constituent Systems
	\item If, and in what way, the Safety-Monitor effectiveness is influenced by the performances of the Controller, in terms of:
	\begin{itemize}
		\item[-] Length of the training period
		\item[-] Strategies adopted during the training
	\end{itemize}
\end{itemize}

The development of this first methodology has taken into account the tools available for this kind of activities. Since most technologies in this field are private and proprietary, we had to look for open-source equivalents, demonstrating that it's not easy to perform these analyses in non-professional environments.

CARLA is with no doubt the best self-driving car simulator available, but it's not perfect. The LiDAR sensor bug\cite{lidarbug} would have prevented us to do this study and if it wasn't for the project developed by Zhuang\cite{carlapro}, it would have been \textsl{unfeasable} because our only alternative would have been to use \textsl{ground-truth} data, which would have been pointless for this work.

If on one hand, \textsl{Coach framework} is developed by \textsl{Intel AI Labs}, it suffers from the common issues often found in open-source project, the most evident being one of the two agents provided for CARLA, which stopped moving after few stages of training in \textsl{phase 1} (therefore unrelated to the use of Safety-Monitor).
This bug makes us wonder about the presence of other issues in the design of the training algorithm which may had an influence on the behaviours observed for $C_{S2\dots 4}$.

Another issue that mines the usability of this framework is that the RAM is not correctly freed, eventually filling it and causing a crash of the running processes. This required us to monitor the training execution and restart it at every crash.\newline

The monitoring activity was developed keeping in mind the main properties of:

\begin{itemize}
	\item Repeatibility
	\begin{itemize}
		\item[-] By recording the Controller's execution in files, saving the action taken by the Controller in each frame and the seeds used in RNGs\footnote{Random Number Generators}, the Destination Goals and the environmental parameter of each scenario
	\end{itemize}
	\item Non-Intrusiveness
	\begin{itemize}
		\item[-] Code instrumentation, as said, may produce non-real time data or wrong measurements, due to the client-server architecture used. CARLA solves this problem by allowing users to run simulations at a \textsl{fixed time step}. However the hardware must be powerful enough to achieve the time step required
	\end{itemize}
	\item Representativeness
	\begin{itemize}
		\item[-] Representativeness of test-cases is one of the central problems when monitoring self-driving cars, because hazardous events may be so complex and so many that is impossible to consider them all. The definition of scenarios and difficulty level helps in this sense, providing many different sitations starting from the same scenario
	\end{itemize}
	\item Feasibility
	\begin{itemize}
		\item[-] The research on the best tools to be used for this activity was not easy, due to the multitude of solutions proposed that in reality have lots of practical limitations. However, the tools used in this work allowed us to perform this activity. At the same time we can not ignore the issues found in these projects
	\end{itemize}
\end{itemize}

If the \textsl{feasibility} property is undermined by the presence of issues in the tools used, the methodology developed still proved to be an effective way of assessing the reliability of Autonomous Vehicles allowing us to approximate all the common measures of interest for these systems, without sticking to common metrics used in the evaluation of neural networks, e.g. the \textsl{Loss Function}, which indeed are powerful and mandatory measures to compute to check that the neural network is actually learning, but doesn't give any evidence at all on the goodness of the System in its entirety.\newline

The main conjecture that made us start this work, was if "static" (in the sense that they don't learn from new data) error-checkers' performances were depending on the performances of neural networks and if ad hoc training strategies have an influence as well.

The failure of those strategies that used the Safety-Monitor to guide the training ($S2,\: S3,\: S4$) did not give us evidences on this fact, and this problem was already discussed in the previous chapter. Moreover, the \textsl{time needed} to train the Controllers prevented us to perform longer training activities \textsl{and} to try different values for the reward parameters.

E` VERO CHE LE FORMULE SONO MATEMATICHE MA NON CI SONO STANDARD PER LA VALUTAZIONE DI QUESTI NELLE AUTONOMOUS VEHICLE

E IN GENERALE SI PROCEDE A TENTATIVI

RIPETERE LE PREMESSE (MONITORING DI VEICOLI AUTONOMI E SAFETYMONITOR - CONTROLLER PROBLEM)

RAPIDAMENTE METODO SPERIMENTALE, COME SONO RISOLTI I PROBLEMI, COSA CI PERMETTE DI FARE

CONSIDERAZIONI SU:

FALLIMENTO DEL MONITOR TRAINING

SVILUPPI FUTURI POTREMO FARE DI PIU`

INTERESSANTE AVERE MODELLI PIU` ESPERTI E STRUMENTI (ALGORITMI) MIGLIORI NON OPEN-SOURCE