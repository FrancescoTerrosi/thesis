\chapter{Conclusions}

In this work we explored the topic of how to perform a monitoring activity on self-driving cars, controlled by a Control System (Controller) and a System Supervisor (Safety-Monitor).

This study is meant to be a first step on the analysis of this kind of activity for such systems, which require particular care due to their complexity, and the critical issues of the environment in which it performs. Autonomous Vehicles themselves are not a new kind of systems, since they are commonly employed in military actions. However, the fact that self-driving cars work at close contact with people and that the urban environment requires extra care in handling the moltitude of events (hazardous or not) that may happen.
Moreover, we questioned if there are relationships between the effectiveness of the Safety-Monitor component and the Controller component, when the latter gets trained for long periods, using different strategies.

In the first sections, we reviewed the main concepts of these activities, such as

\begin{itemize}
	\item The definition of \textsl{dependability} and \textsl{safety}, which are central to Safety-Critical Systems, are seen in the light of self-driving cars
	\item The problems that prevent us to deploy self-driving cars in urban environments at this level of knowledge
	\item How a wrong/poor/optimistic \textsl{dependability assessment} of these systems may have tragic consequences when these systems are deployed
	\item The problems of studying the observed emergent behaviour resulting from the interaction of two main Constituent Systems
	\item If, and in what way, the Safety-Monitor effectiveness is influenced by the performances of the Controller, in terms of:
	\begin{itemize}
		\item[-] Length of the training period
		\item[-] Strategies adopted during the training
	\end{itemize}
\end{itemize}

The development of this first methodology has taken into account the tools available for this kind of activities. Since most technologies in this field are private and proprietary, we had to look for open-source equivalents, demonstrating that it's not easy to perform these analyses in non-professional environments.

CARLA is with no doubt the best self-driving car simulator available, but it's not perfect. The LiDAR sensor bug\cite{lidarbug} would have prevented us to do this study and if it wasn't for the project developed by Zhuang\cite{carlapro}, it would have been \textsl{unfeasable} because our only alternative would have been to use \textsl{ground-truth} data, which would have been pointless for this work.

If on one hand, \textsl{Coach framework} is developed by \textsl{Intel AI Labs}, it suffers from the common issues often found in open-source project, the most evident being one of the two agents provided for CARLA, which stopped moving after few stages of training in \textsl{phase 1} (therefore unrelated to the use of Safety-Monitor).
This bug makes us wonder about the presence of other issues in the design of the training algorithm which may had an influence on the behaviours observed for $C5_{S2\dots 4}$.

Another issue that mines the usability of this framework is that the RAM is not correctly freed, eventually filling it and causing a crash of the running processes. This required us to monitor the training execution and restart it at every crash.\newline

The monitoring activity was developed keeping in mind the main properties of:

\begin{itemize}
	\item Repeatibility
	\begin{itemize}
		\item[-] By recording the Controller's execution in files, saving the action taken by the Controller in each frame and the seeds used in RNGs\footnote{Random Number Generators}, the Destination Goals and the environmental parameter of each scenario
	\end{itemize}
	\item Non-Intrusiveness
	\begin{itemize}
		\item[-] Code instrumentation, as said, may produce non-real time data or wrong measurements, due to the client-server architecture used. CARLA solves this problem by allowing users to run simulations at a \textsl{fixed time step}. However the hardware must be powerful enough to achieve the time step required
	\end{itemize}
	\item Representativeness
	\begin{itemize}
		\item[-] Representativeness of test-cases is one of the central problems when monitoring self-driving cars, because hazardous events may be so complex and so many that is impossible to consider them all. The definition of scenarios and difficulty level helps in this sense, providing many different sitations starting from the same scenario
	\end{itemize}
	\item Feasibility
	\begin{itemize}
		\item[-] The research on the best tools to be used for this activity was not easy, due to the multitude of solutions proposed that in reality have lots of practical limitations. However, the tools used in this work allowed us to perform this activity. At the same time we can not ignore the issues found in these projects
	\end{itemize}
\end{itemize}

If the \textsl{feasibility} property is undermined by the presence of issues in the tools used, the methodology developed still proved to be an effective way of assessing the reliability of Autonomous Vehicles allowing us to approximate all the common measures of interest for these systems, without sticking to common metrics used in the evaluation of neural networks, e.g. the \textsl{Loss Function}, which indeed are powerful and mandatory measures to compute to check that the neural network is actually learning, but doesn't give any evidence at all on the goodness of the System in its entirety.

This methodology proved to be an effective mean to study the behaviour of the whole System, focusing on well-known measures used to assess a system's dependability and the Safety-Monitor's performances. The definition of \textsl{Scenarios} and \textsl{difficulties} is very useful in the understanding of what environmental parameters cause a degradation on System's performances and in the generation and analysis of \textsl{"uncommon"} situations that not always can be taken into account when designing \textsl{ad-hoc safety-cases}
\newline

The main conjecture that made us start this work, was if "static" (in the sense that they don't learn from new data) error-checkers' performances were depending on the performances of neural networks and if ad hoc training strategies have an influence as well. From the data collected it looks like the Monitor's effectiveness is likely to change with expert agents, motivating us to continue to study in deep this conjecture.

The failure of those strategies that used the Safety-Monitor to guide the training ($S2,\: S3,\: S4$) did not give us evidences on this fact, and this problem was already discussed in the previous chapter. This failure may be caused by two factors:

\begin{itemize}
	\item[1)] Misspecified reward function
	\item[2)] The training period was not long enough
\end{itemize}

The fact that the agents trained were not getting unintentionally high reward for standing still, makes us think that the neural network needed more time to learn what the Safety-Monitor is trying to teach. A higher \textsl{simulation time step} could have helped us, making \textsl{simulation time} running faster than \textsl{real time}. However, this required ultra-high performance hardware, therefore we we could not train the models for more time, nor retrain the networks adjusting the reward parameters.

At the same time, the Safety-Monitor proved to be \textsl{less} effective when operating in combination with the Checkpoint which had the \textsl{best} performances and, most of all, was obtained from Checkpoint $C4$ by changing the training strategy, while when operating with the Checkpoints trained with strategy $S0$ $(C1,\: C2,\: C3,\: C4,\: C5_{S0})$ it had the same effectiveness with all of them ($0.75$ false positive rate) except for $C2$. If on one hand, $C2$ was definetely a special case, for the reasons listed in the previous chapter, at the same time we can not exclude the same hypothesis for $C5_{S1}$. In future works, we could continue the training of $C5_{S1}$ to confirm this fact.\newline\newline

This work demonstrated the power of the use of a simulated environment for the assessment self-driving cars, taking into account and solving many of the problems found when approximating the measures of interest for these systems, and common problems found in a monitoring activity.

In future works, our interest will be:

\begin{itemize}
	\item Develop stochastical models based on data collected with this monitoring methodology, to guide us in the tunning of the reward function parameters
	\item Continue the training of $C5_{S0\dots 4}$ to:
	\begin{itemize}
		\item[a)] Collect more evidences about the effectiveness of the Safety-Monitor
		\item[b)] Have a better understanding of the failure causes of strategies $S2\dots 4$
	\end{itemize}
	\item Apply the same monitoring methodology using more powerful tools (e.g. proprietary software, pretrained neural networks, better Safety-Monitor)
\end{itemize}
