\chapter{System Analysis Method}

In this chapter is presented a method to study the safety level of an autonomous car over time, observing the emergence resulting from the interactions of a neural network controller and a safety monitor in a simulated environment.\newline
A neural network was trained, tested and trained again several times with and without the safety monitor, to collect data about the emergence of these components.

\section{Experiments Preparation and Measures}

The goal of this work is to develop and to assess the feasability of an experimental method for the safety assessment of an autonomous vehicle. Due to the system being composed by two constituent systems: the Controller and the Checker, we think that a point of view based on the emergent behaviour resulting from the interaction of these systems can improve the quality of the assessment.\newline

The proposed framework is designed with particular attention on studying the emergence resulting from the interaction of these two consitutent systems. The main aspects we are interested in are:

\begin{itemize}
	\item How the coverage of the system changes when the same monitor is applied to different stages of a network
	\item Changes in the safety gain provided by the same monitor when applied to different networks
	\item What features of the monitor determine an improvement (or worsening) to the safety of the system
	\item What impacts the use of a safety monitor has on the training of a neural network
	\item Possible behaviours of the neural network having an impact on the monitor usefulness
\end{itemize}

At the system level, we are interested in the probability of a safety-failure (e.g. a crash) and how to minimize it, or in the safety of the system as a result of the training of a neural network, the Controller and a fault tolerance mechanism, the Monitor. As long as the network is trained properly, we expect that $P_{C_{i}}(failure) \geq P_{C_{i+t}}(failure)$ where $C_{i}$ represents a neural network controller trained for $i$ epochs. At the same time we want our safety monitor to provide at least the same level of safety if the same network is trained again for $t$ epochs.\newline
The analysis must start with the definition of $n_{l=0\dots h}$ safety cases, where $n$ is the effective number of cases and $h$ is used to describe the \textsl{"level of difficulty"} of the simulated environment. $C_{i}$ is tested in all the scenarios, starting with conditions somehow \textsl{"favorable"} to the system ($h=0$) and then increasing the difficulty (e.g. increasing the traffic in the scenario and/or simulating a bad weather). This method allows to observe the \textsl{Time To Failure} of the Controller under diverse points of view:


MISURE PER ORA MESSE QUI, DA RIGUARDARE


\begin{itemize}
	\item $TTF_{i,j_{l}}$ as the time in which the controller at stage $i$ fails in the ${j_{l}}^{th}$ scenario
	\item $MTTF_{i,l}$, mean time to failure when the system performs at level of difficulty $l$
	\item $MTTF_{Controller_{i}} = \frac{1}{n} \sum_{k = 0}^{n} MTTF_{i,k}$
\end{itemize}

The controller is then trained again and re-tested in the same scenarios.

Since we are working in a simulated environment, the time to failure was computed using simulation steps, without loss of generality.\newline 

-------------------------------------------------------------------        FATTO CHE PROB INCIDENTE SOMMI A 1. COME LO ESPRIMO?\newline

One of the main problem realated to assessing AVs' safety is the execution time. The probability of a failure increases monotonically over time, but the increasing factor should be lower the more the network is trained, so variation on the hardness of scenarios must be designed accurately. This property could result in very long simulations, but it also gives a useful hint for checking whether the system's safety is improving or not. \newline\newline
Once trained neural networks become essentially black boxes and even a small variation on the training parameters could result in totally different behaviours during test phase, therefore it can not be assumed that the same software (the monitor) will provide the same level of safety.\newline
The monitor must be tested in the same scenarios in which the controller was tested and should not intervene during the simulation, but at the rigth time needed to prevent the failure event if the controller failed the scenario. The simulations previously recorded are now repeated with the monitor activated and the following measures are extracted:

\begin{itemize}
	\item True Positive Rate (or \textsl{Sensitivity)})
	\begin{itemize}
		\item Rate of successful detections where the controller failed.
	\end{itemize}
	\item False Positive Rate (or \textsl{Fall-out})
	\begin{itemize}
		\item Rate of the events in which there's no failure but Monitor raises alarm. It is important that the fault-tolerance mechanism is not activated or the simulation will be compromised
	\end{itemize}
	\item False Negative Rate (or \textsl{Miss Rate})
	\begin{itemize}
		\item Rate of failures in the controller not detected by the monitor
	\end{itemize}
\end{itemize}

Of course we desire the monitor's detections to be the most accurate possible. For this reason \textsl{Sensitivity}\footnote{True Positive rate: the proportion of safety measures applied by the monitor when actually needed} and False Negative rate were chosen as measures of interest.\newline
While in most of the cases a false positive will result in a state of degraded service, since a self-driving car is a safety-critical system performing in a dynamic environment, a false positive could put the system in an unsafe state (imagine performing an emergency brake for no reason on the highway), so False positive rate was taken into account as well.\newline
However, these measures themselves aren't sufficient to detect changes in the interaction between the two CSs, so it's useful to record data such as the vehicle's speed and the controller's throttling/steering and to combine them with data recorded from the monitor to detect possible correlations between the behaviours.
These values must be recomputed not only when the Monitor is improved (either by implementing a more sophisticated detection method or by improving the quality of sensors) but also when the network is trained because/due to...
=============================================================================================
GIUSTIFICARE QUI IL DISCORSO FAMOSO O RIMANDARE A CAPITOLO PRECEDENTE SU LETTERATURA?
==============================================================================================

After collecting the data we need, performances can be easily compared to assure that the use of the same monitor with 2 different networks (or the same network in 2 different epochs) doesn't become detrimental with more expert Controllers.

\section{Experimental methodology}

In this section we propose and describe the methodology developed in this work.\newline

The study consists of several experiments in a simulated realistic environment, in which we observe how the coverage of the safety-monitor (i.e. the probability of raising an alert if there really is a safety-hazard) varies with respect to a neural network \textsl{in different stages of training}.\newline
The definition of the $n_{0}$ safety cases are the first step to perform the analysis. Each scenario should be different in terms of initial conditions for the System, but the environment should be identical. For example, given that in a situation of regular traffic the amount of cars is at most $m$, all the $n_{0}$ scenarios should spawn $m$ cars, but the initial state of the system should change (e.g. by changing the starting point). Now, after defining $h$ hardenings of the, $n_{1\dots h}$ variations of the scenarios are generate (note that here $n$ is fixed). It is important that everything else but the variation specifically designed is identical. For such reason each stage $j_{k\neq 0}$ all the parameters of the simulation should be identical to those in the stage $j_{0}$, such as \textsl{seeds} used in pseudo number generators and in general all the parameters not changed by the $k_{th}$ variation.\newline
The design and definition of the safety cases and the levels of difficulty are subjected to a lot of factors, and it is impossible to cover all the situations that may happen in a real environment. By repeating the same safety cases in worse conditions, we have a better idea of the system's ability to handle unattended situations than creating a specific safety case for the bad condition itself because in such way it's possible to study possible correlations between failures and environmental conditions.\newline\newline
\newpage


After the definition of the scenarios, the constituent systems are tested separately to get statistics about the interactions between the two, these data are then compared to those in which the Controller and the Monitor cooperate to fullfill the assigned task.\newline
The testing phase is organized in the following steps:

\begin{itemize}
	\item[1] Controller testing phase
	\item[2] Monitor testing phase
	\item[3] Data Analysis
	\item ======================================================
	\item[?] System testing phase
	\item[?] Data Analysis
\end{itemize}

In step $1$, the controller is tested alone in the $(h+1)\cdot n$ scenarios. In addition to the TTFs as defined above, additional data are collected. This data are used to enrich the understanding of the mistakes in the control system. Examples of this data are the vehicle's speed, the controller's action (throttling, steering or braking), if there was a crash: was it with another vehicle? With a pedestrian? The data collected are then aggregated to assess the controller's behaviour.\newline
Since the design and especially the testing of a safety-critical system is a circular process and can not end after an acceptance test phase, informations collected in this first step can be used to guide the hardening factors in novel experiment suites.\newline
One of the most critical parameter when designing the scenarios in which the controller will be tested, is the definition of alting criteria: we don't know how long the car must drive before eventually crashing. However this problem is still unsolved and it's let to the user to define \textsl{reasonable} criteria to stop the execution of a scenario. However, we think that whenever a collision is detected the scenario should be considered concluded.

========================================================================================
Credo che questa cosa sia importante. Visto che lavoriamo nell'assunzione (almeno noi) che la macchina si schianti, merita dire di trovare i parametri che rendano "abbastanza" difficile?   ----> si ricollega anche al fatto di uno goal?

========================================================================================
\newline

It is important that in this step, the \textsl{exact} behaviour is recorded for repeatability, as a sort of black box. This is because a neural networking tested in two execution of the \textsl{same} scenario with the \textls{same} initial condition could take slightly different decisions that would change the rest of the environmental conditions.\newline\newline

=============================================

DISEGNINI PRIMA FASE

=============================================

In the second step the positives/negatives rates of the Monitor are studied. Our goal is to measure the coverage factor of this component with respect to the failures of the Controller.\newline
The \textsl{executions} of the simulations previously recorded are repetead in all the $j_{l}; \: j=1\dots n, \: l=0\dots h$ scenarios, attaching the Monitor to the system. For each scenario, the alerts raised by the Monitor are recorded but the fault-tolerance mechanism is not activated until the instant of time $z$ needed to prevent the failure event. The choice of $z$ is a sensitive issue for the accuracy of the measuresements: if the Monitor is activated too late, the failure will not be prevented and it's not possible to record just the alert raised (if any), since the efficacy of the safety procedure is an indicator of the Monitor's performance. At the same time, if the Monitor is activated too early, a premature alert will change the conditions in which the failure occurred. In this case the system may avoid the failure event, but it's impossible to say if the system was "good enough" to avoid it or if an early stopping of the car caused by a safty brake prevented it.

Using this approach, if the simulation environment and the hardware are powerful enough, it's possible to have a better understanding on which cases are easily handled by the Controller and not by the monitor, and vice versa.
Given a Controller $C_{i}$ and a Monitor $M$, we calculate the followings:

\begin{equation}
	FPR = \frac{FP}{FP+TN}
\end{equation}
\begin{equation}
	TPR = \frac{TP}{TP+FN}
\end{equation}
\begin{equation}
	FNR = \frac{FN}{FN+TP} = 1 - TPR
\end{equation}

As pointed before, any alert raised before time $z$ are recorded as False Positives. This allows to effectively count the number of false alarms in a given scenario, which can then be easily aggregated with the results from all the simulations to compute a good estimate of the False Positive Rate.\newline
With a good simulator it's possible to check in every frame if a crash happened and, possibly, with what the car collided. With this information, the time $t$ in which there was a crash can be easily extracted. Assuming a \textsl{proper choice} of $z$, allowing the Monitor to intervene in the time interval $t-z$ allows us to check whether the Monitor correctly detected the hazard or not.\newline
These values calculated over the sets of the $n$ scenario, for all the $h$ difficulties, are useful to understand once again what kind of situations could put the system at risk, looking at the interactions between these two components. Specifically, with this approach the coverage of the 2 constituent systems is studied in two different steps but with in the same environment(s), therefore is easier to look at similarities between failures of the controller (crashes) and failures of the Monitor (false negatives), if any.\newline\newline
It is reasonable to think about different difficulty levels $0\dots h'$ for the Safety Monitor as well. A value of $h' = 0$ may be considered as using the Monitor as it is intended to be mounted on the system. Defining $n'$ as $(h+1)\cdot n$, the number of scenarios in which the Controller was tested autonomously, the monitor is tested under $h'$ operating conditions, creating $(h' + 1) \cdot n'$. Examples of perturbations on the monitor may include downgrading of the sensors' quality or noise injection in the data collected.\newline\newline
